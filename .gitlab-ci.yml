variables:
  IMAGE_NAME: funkwhale/funkwhale
  IMAGE: $IMAGE_NAME:$CI_COMMIT_REF_NAME
  IMAGE_LATEST: $IMAGE_NAME:latest
  PIP_CACHE_DIR: "$CI_PROJECT_DIR/.pip"
  CARGO_HOME: "$CI_PROJECT_DIR/.cargo"
  PYTHONDONTWRITEBYTECODE: "true"
  REVIEW_DOMAIN: preview.funkwhale.audio
  REVIEW_INSTANCE_URL: https://funkwhale.juniorjpdj.pl
  DOCKER_HOST: tcp://docker:2375/
  DOCKER_DRIVER: overlay2
  DOCKER_TLS_CERTDIR: ""
  DOCKER_BUILD_PLATFORMS: linux/amd64,linux/arm64,linux/arm/v7
  PIP_EXTRA_INDEX_URL: ${CI_API_V4_URL}/projects/${CI_PROJECT_ID}/packages/pypi/simple

workflow:
  rules:
    - if: $CI_PIPELINE_SOURCE == "merge_request_event"
    - if: $CI_COMMIT_BRANCH && $CI_OPEN_MERGE_REQUESTS
      when: never
    - if: $CI_COMMIT_BRANCH

stages:
  - deploy
  - lint
  - test
  - deps_build
  - build
  - publish

.time_script: &time_script
  |
  # Timer setup
  procstarttime=$(date -u '+%s')
  
  s_timestamp() {
    while read -r line; do
      curtime=$(date -u '+%s')
      deltatime=$(( curtime - procstarttime ))
      timestamp="$(printf "%03d:%02d" $(( deltatime / 60 )) $(( deltatime % 60 )))"

      # by nature BASH might run process subst twice when using >&2 pipes. This is a lazy
      # way to avoid dumping two timestamps on the same line:
      if [[ "$line" != \[${timestamp%% *}* ]]; then
          echo "[$timestamp] $line"
      else
          echo "$line"
      fi
    done
  }

  exec 1> >(s_timestamp)
  exec 2> >(s_timestamp)
  echo Timer start!

build_dep_wheels:
  rules:
    - if: $MANDATORY == "false"
      allow_failure: true
    - allow_failure: false
  stage: deps_build
  tags: [docker, $DOCKER_ARCH]
  
  image: ${DOCKER_ARCH}/alpine:${ALPINE_VERSION}
  parallel:
    matrix:
      - DOCKER_ARCH: [amd64, i386, arm64v8, arm32v7]
        ALPINE_VERSION: "3.17"
      - DOCKER_ARCH: [riscv64]
        ALPINE_VERSION: "edge"
        MANDATORY: "false"
  cache:
    - key: dep-cargo
      paths:
        - .cargo/registry/index
        - .cargo/registry/cache
        - .cargo/git/db
    - key: dep-pip-$DOCKER_ARCH
      paths:
        - .pip
  before_script:
    - >
      apk add
      alpine-sdk
      python3-dev
      postgresql-dev
      libffi-dev
      zlib-dev
      jpeg-dev
      openldap-dev
      openssl-dev
      cargo
      libxml2-dev
      libxslt-dev
      poetry
      py3-pip
      patchelf
    - pip install auditwheel twine
    - cd api ; poetry export --with dev --without-hashes > ../requirements.txt ; cd ..
    - python -m venv venv
    - venv/bin/pip install --upgrade pip wheel
    - venv/bin/pip debug --verbose
  script:
    # show time stats
    - *time_script
    # build basic wheels
    - venv/bin/pip wheel --check-build-dependencies -w wheelhouse -r requirements.txt
    # remove universal wheels (those are probably downloaded from pypi or our repo)
    - rm -f wheelhouse/*-none-any.whl wheelhouse/*-musllinux_*.whl
    - ls -l wheelhouse
    - |
      # correct the wheel tagging needed to workaround maturin and other 3-rd party packagers problems on multi-arch envs..
      # https://github.com/PyO3/maturin/issues/1289 but appears on other arches and packagers too
      for wheel in wheelhouse/* ; do
        [ "$wheel" = 'wheelhouse/*' ] && continue
        echo "Correcting tags for $wheel"
        corrected_tag="$(python3 -c "import auditwheel.wheel_abi; print(auditwheel.wheel_abi.analyze_wheel_abi('$wheel').overall_tag)" || true)"
        [[ "$wheel" = "*$corrected_tag*" ]] && { echo 'Tag already correct or correct tag is unknown - skipping.' ; continue ; }
        echo "New tag: $corrected_tag"

        venv/bin/wheel unpack -d wheeltmp "$wheel"
        sed -ri 's/^(Tag: .*)-.*?$/\1-'"$corrected_tag"'/g' wheeltmp/*/*.dist-info/WHEEL
        rm -f "$wheel"
        venv/bin/wheel pack -d wheelhouse wheeltmp/*
        rm -rf wheeltmp
      done
    - |
      # prepare musllinux universal wheels
      for wheel in wheelhouse/* ; do
        [ "$wheel" = 'wheelhouse/*' ] && continue
        echo "Fixing $wheel"
        auditwheel repair --strip "$wheel" || true
      done
    # remove basic wheels as we have universal wheels now
    - rm -f wheelhouse/*-linux*.whl wheelhouse/*.linux*.whl
    - ls -l wheelhouse
    - |
      # upload wheels to gitlab package repository
      for wheel in wheelhouse/* ; do
        [ "$wheel" = 'wheelhouse/*' ] && continue
        echo "Uploading $wheel"
        TWINE_USERNAME=gitlab-ci-token TWINE_PASSWORD=${CI_JOB_TOKEN} twine upload --disable-progress-bar --repository-url "${CI_API_V4_URL}/projects/${CI_PROJECT_ID}/packages/pypi" "$wheel" || true
      done
  artifacts:
    expire_in: 2 weeks
    paths:
      - wheelhouse
